{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Second DayResults.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehmetalianil/epoch-15/blob/master/Second_Day.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fpMCI8yA47p",
        "colab_type": "text"
      },
      "source": [
        "# First day\n",
        "\n",
        "This day, we are going to do the following: \n",
        "* If not done yet, we need to sort out some prerequisites.\n",
        "* We are going to generate a dataset of hand gestures, together.\n",
        "* We might spend some time to gather and preprocess the data. (Never underestimate time spent for data-prep)\n",
        "* Then we will start a Tensorflow session, and try to solve a classification problem of classifying these gestures correctly.\n",
        "\n",
        "## Resources\n",
        "* Arduino IDE download link https://www.arduino.cc/en/Main/Software\n",
        "* Linux CH340 drivers patching guide https://learn.sparkfun.com/tutorials/how-to-install-ch340-drivers/all#linux\n",
        "* Upload point for generated data: https://drive.google.com/drive/folders/138hSEWZyePWepjaOIrhxuYEvBwqLl2PM?usp=sharing\n",
        "* A VERY helpful Colab written by FranÃ§ois Chollet, developer of KERAS https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO\n",
        "* Some numpy/pandas commands that might be useful https://elitedatascience.com/python-cheat-sheet\n",
        "* We will be working with numpy arrays in high dimension. Here is how to slice them. https://www.pythoninformer.com/python-libraries/numpy/index-and-slice/\n",
        "\n",
        "## Introduction\n",
        "\n",
        "*There might be new faces in this session, therefore I would like to write an introduction.*\n",
        "\n",
        "I am Mehmet Ali Anil, co-founder of Grus in High Tech Campus. I am a electrical engineer and physicist with particular interest on biologically inspired systems for a while now. Here are the places that you can contact me and read things I write: \n",
        "\n",
        "* Github https://github.com/mehmetalianil\n",
        "* StackOverflow https://stackexchange.com/users/462542/mehmet-ali-anil\n",
        "* LinkedIn https://www.linkedin.com/in/mehmetalianil/\n",
        "* Our blog https://blog.grusbv.com/\n",
        "* Twitter https://twitter.com/maanil_ee \n",
        "* Podcast {{Work In Progress}}\n",
        "\n",
        "## Why microcontrollers? Isn't ML the job for, like endless server farms?\n",
        "It doesn't have to be. It might have been a bad idea before, but the unexpected success of deep learning on specific problems has laid the groundwork for its widespread applicability. And systems with small resources, given a small problem to solve excel in applications that require: \n",
        "\n",
        "* Autonomy\n",
        "* Privacy\n",
        "* Speed (as in latency)\n",
        "* Energy efficiency\n",
        "\n",
        "In short, life is what is here and now, and why not solve problems where they are?\n",
        "\n",
        "There are many ways to incorporate machine learning in embedded devices, but I am personally intrigued by the idea of using microcontrollers that are essentially ubiquotus now. [This Quora link](https://www.quora.com/How-many-embedded-systems-are-there) states that there might be 75-100 billions of devices housing microcontrollers. Due to their nature, these microcontrollers are used for tasks that involve deterministic control. They are so cheap and widespread that they might just as well be hardware neurons in our designs. We just need to learn how to program them. That is what we are up to. \n",
        "\n",
        "## We have some prerequisites.\n",
        "\n",
        "### Get a serial monitor\n",
        "\n",
        "We need a serial terminal to get our data out of our sparkfun boards. There are a few candidates:\n",
        "\n",
        "* Docklight\n",
        "* Tera Term\n",
        "* screen (Linux terminal)\n",
        "* Arduino IDE (I tested it here)\n",
        "\n",
        "I have chosen Arduino because it also has a plotter that graphs the serial data stream if it has comma separated values. \n",
        "\n",
        "### If using Linux, download patched CH340 drivers\n",
        "The current driver in the Linux kernel needs to be patched before programming the device with high speeds around 1MHz. It might be okay today, since we will only program if we have the time, but keep it in mind, follow the guidelines here:\n",
        "\n",
        "https://learn.sparkfun.com/tutorials/how-to-install-ch340-drivers/all#linux\n",
        "\n",
        "## Lets generate data\n",
        "\n",
        "* Plug the USB cable to the computer.\n",
        "* Plug the serial converter board on, the dark board.\n",
        "* Plug the red Sparkfun Edge board on the serial converter board. They should both be chip side up. (The sparkfun board has a GRN written on the top pin,  which is **not** and indication that it must be connected to GND on the serial converter.)\n",
        "* Your board might have some of its lights on, don't worry. \n",
        "* Fire up Arduino IDE, `Ctrl`+`Shift`+`L` or `Tools>>Serial Plotter` to fire up the serial plotter. \n",
        "* Press the button on your board marked as RST while watching the Serial Plotter. Sparkfun edge board will send a record of 2 seconds woorth of accelerometer readings.\n",
        "* Experiment with the accelerometer if you are into that sort of things. Check whether gravity is still 1G. \n",
        "* Hold the boards secure in your hand (you can hold by the serial board) and come up with a hand gesture that you can do within a second, something bold and makes a distinct waveform on the serial plotter. Start and stop without jerking the device. Practice here. (Each person, one gesture.)\n",
        "* Name this gesture, draw it up on the board to avoid different people choosing very similar gestures.\n",
        "* Close the serial plotter, fire up the serial monitor with `Ctrl`+`Shift`+`M` or `Tools>>Serial Monitor`\n",
        "* Now, record as many gestures as you can. Press the RST button on the device, and execute the gesture. Try to get the gesture into the middle of the 2 minute window. Try to have a single way of holding the boards, start and stop without motion. \n",
        "* If there are some spurious signal, it is due to the cable and the connections. check them out and continue. I will clean them out by hand.\n",
        "* After done, copy paste the output into a file named for your gesture, with the extension .csv. It should be like: \"fistbump.csv\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItqYkJ0AAdG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this command in order to download or update the dataset that we are building up from scratch!\n",
        "!rm -rf epoch-15/\n",
        "!gsutil -m cp -r gs://epoch-15 ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYwIgAQK_ti",
        "colab_type": "text"
      },
      "source": [
        "##Enabling GPU in Colab\n",
        "\n",
        "We need to be sure that we are using TF 2.0 and we would like to use the GPU instance of Google.\n",
        "\n",
        "For that, we select `Runtime >> Change runtime type >> HW accelerator >> GPU`\n",
        "\n",
        "We also instantiate tensorboard here, because it is nicer to have it on the top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liGO59hzKuru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This uninstalls default tensorflow and tensorboad in a Colab and installs the latest nightly snapshots. \n",
        "#!pip uninstall -y tensorflow tensorflow-gpu tensorboard\n",
        "#!pip install  -q tfa-nightly tf-nightly==2.1.0.dev20191029 tb-nightly==2.1.0a20191029 tf-nightly-gpu==2.1.0.dev20191029\n",
        "#!pip uninstall -y tensorflow tensorflow-gpu tensorboard\n",
        "#!pip install -q  grpcio==1.24.3\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "import tensorboard as tb\n",
        "print(tb.__version__)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si6dYffvLqsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To check whether we are enjoying a GPU. Thanks G!\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf29GxtSMhm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "import tensorboard as tb\n",
        "print(tb.__version__)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAfuaVLTpEsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qan5u7J7pFK-",
        "colab_type": "text"
      },
      "source": [
        "#Data preparation\n",
        "\n",
        "Here, I have prepared most that is required. We are going to scan large .csv files that we have generated and get them into shape that we can use. We have an uknown number of samples. Let's call them `Nsamples`. Each sample has 3 \"channels\", ax,ay and az, and each channel has 1024 samples. \n",
        "\n",
        "We are looking at a sample that has a shape of (1024,3). So our dataset will have a shape of (Nsample,1024,3).\n",
        "\n",
        "In Tensorflow, we prepare the data like this, since TF has its own facilities in preparing the data for training. For example, TF cen get this large dataset and jumble it, alter it, divide it up and create batched out of it. \n",
        "\n",
        "Here this script scans our folder and creates a dataset. Data is a Numpy array named `array_data`, the corresponding labels are named `array_labels`. If you need the labels as ints, `array_labels_ints`. If you want them as vectors, `array_data_categorical`.\n",
        "\n",
        "I have thrown in a random plot for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH_kkYlbN9Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "PATH_DATA = \"epoch-15\"\n",
        "SAMPLE_SIZE = 256\n",
        "MAXIMUM_ACCELERATION=32000 #[mG]\n",
        "\n",
        "array_data = np.empty([0,SAMPLE_SIZE,3])\n",
        "array_label = np.empty([0])\n",
        "for root, dirs, files in os.walk(PATH_DATA):\n",
        "    for file in files:\n",
        "        print(\"Reading {}\".format(os.path.join(root,file)))\n",
        "        with open(os.path.join(root,file)) as f:\n",
        "          for sample_counter, chunk in enumerate(pandas.read_csv(f, sep=',',comment='#', error_bad_lines=False, chunksize=SAMPLE_SIZE+1)):\n",
        "              trimmed_chunk = np.expand_dims(chunk.iloc[:-1],axis=0) # has dimensions (1024,3) should be (1,1024,3)\n",
        "              array_data = np.append(array_data,trimmed_chunk,axis=0).astype(np.float32) # We omit the ax,ay,az row\n",
        "              array_label = np.append(array_label,file.split('.')[0])\n",
        "\n",
        "array_data=(array_data.astype(float)+MAXIMUM_ACCELERATION/2)/MAXIMUM_ACCELERATION  # normalize to (0,1)\n",
        "GESTURE_LIST = list(set(array_label))  ## Warning, this won't preserve order. Might find a better (manual?) way.\n",
        "GESTURE_LIST_INT = range(len(GESTURE_LIST))\n",
        "array_label_ints = [GESTURE_LIST.index(label) for label in array_label]\n",
        "array_label_categorical = to_categorical(array_label_ints).astype(np.float32)\n",
        "\n",
        "## Some random graph\n",
        "sample = random.randint(1, len(array_label))\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.plot(np.arange(SAMPLE_SIZE),array_data[sample,:,0])\n",
        "plt.plot(np.arange(SAMPLE_SIZE),array_data[sample,:,1])\n",
        "plt.plot(np.arange(SAMPLE_SIZE),array_data[sample,:,2])\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Acceleration [mG]\")\n",
        "plt.title(array_label[sample]+\" #\"+str(sample))\n",
        "\n",
        "TfLiteFlattened = []\n",
        "for ctr,datapoint in enumerate(array_data[sample,:,0]):\n",
        "  TfLiteFlattened.append(datapoint,)\n",
        "  TfLiteFlattened.append(array_data[sample,ctr,1])\n",
        "  TfLiteFlattened.append(array_data[sample,ctr,2])\n",
        "print(array_label_categorical.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFqEPvZg5NsP",
        "colab_type": "text"
      },
      "source": [
        "Here, we can spend a lot of time. This is where we do some data science. \n",
        "We need to come up with a model that successfully categorizes these gestures. It means, we are going to determine: \n",
        "* Individual layers (Conv1D, Dense, Flatten, etc)\n",
        "* Activation Functions (ReLU, Softmax, etc.)\n",
        "* Data modifications (noise, augmentation, cropping)\n",
        "* Optimizer\n",
        "* Error function and metrics\n",
        "\n",
        "It cannot be too large, since we have a limited FLASH memory in out small board. \n",
        "\n",
        "Try to come up with a successful model running on the cloud (here!) and we might embed it into the our boards. I have given you a very simple one here.\n",
        "\n",
        "Check the Tensorboard up in this page. It will update the latest results back from your training. \n",
        "\n",
        "Following challenges:\n",
        "* How is your training accuracy and validation accuracy? Is there room for better?\n",
        "* How does changing BATCH size and EPOCH count effect the training?\n",
        "* How can we make our model robust?\n",
        "* Is there a way to find out which gestures are mistaken for each other? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejeXmYXflBsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime\n",
        "\n",
        "\n",
        "log_dir=\"logs/fit/model1\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "file_writer = tf.summary.create_file_writer(log_dir)\n",
        "array_data = np.expand_dims(array_data,axis=3)\n",
        "#array_label_categorical = np.expand_dims(array_label_categorical,axis=2)\n",
        "(trainX, testX, trainY, testY) = train_test_split(array_data,array_label_categorical,test_size=0.25, random_state=42)\n",
        "print(\"Shapes: trainX {} testX {} trainY {} testY {}\".format(trainX.shape, testX.shape, trainY.shape, testY.shape))\n",
        "\n",
        "# \n",
        "#  CHALLENGE come up with a better model!\n",
        "#\n",
        "\n",
        "## MAA Simplified\n",
        "#model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(SAMPLE_SIZE, 3)))\n",
        "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(len(GESTURE_LIST), activation='softmax')) # softmax is used, because we only expect one gesture to occur per input\n",
        "model.summary()\n",
        "\n",
        "#model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(trainX, trainY, epochs=1000, batch_size=100, validation_data=(testX, testY),callbacks=[tensorboard_callback])\n",
        "## This works for batch size 1 but somehow not for larger batches. Can we check?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcz_8NHGg0xF",
        "colab_type": "text"
      },
      "source": [
        "# Second Day\n",
        "\n",
        "Now, we are going to convert our Tensorflow model into a TF Lite model, and get something that we can put into our devices.\n",
        "\n",
        "We generate a representative dataset that the converter uses to optimize the weight in such a way that we lose the least accuracy. \n",
        "\n",
        "Here I use the test dataset, but if I had a valication dataset, that would have been a better choice, or I could have used the whole dataset for generalizability. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USZHbnJFvwV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "## Let's print the \n",
        "print(np.expand_dims(testX[0].astype(np.float32),axis=0).shape)\n",
        "\n",
        "def representative_dataset_generator():\n",
        "  for value in testX:\n",
        "    # Each scalar value must be inside of a 2D array that is wrapped in a list\n",
        "    yield [np.expand_dims(value.astype(np.float32),axis=0)]\n",
        "converter.representative_dataset = representative_dataset_generator\n",
        "## represantative dataset generator requires a lits of one item of shape (1,N,3)\n",
        "tflite_model = converter.convert()\n",
        "open(\"gesture_quantized.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "quantized_model_size = os.path.getsize(\"gesture_quantized.tflite\")\n",
        "print(\"Model is normally {} bytes, but {} bytes when quantized\".format(basic_model_size,quantized_model_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQgbpzOk0IWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0sp1eO-agqd",
        "colab_type": "text"
      },
      "source": [
        "Let's explore our quantized dataset. \n",
        "Check different layers and how they are interpreted. \n",
        "\n",
        "Check the graph, how much accuracy did we lose?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd3dv7RBSbMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pprint\n",
        "# Instantiate an interpreter for each model\n",
        "gesture_model = tf.lite.Interpreter('gesture_model.tflite')\n",
        "gesture_model_quantized = tf.lite.Interpreter('gesture_quantized.tflite')\n",
        "\n",
        "# Allocate memory for each model\n",
        "gesture_model.allocate_tensors()\n",
        "gesture_model_quantized.allocate_tensors()\n",
        "\n",
        "# Get indexes of the input and output tensors\n",
        "gesture_model_input_index = gesture_model.get_input_details()[0][\"index\"]\n",
        "gesture_model_output_index = gesture_model.get_output_details()[0][\"index\"]\n",
        "gesture_model_quantized_input_index = gesture_model_quantized.get_input_details()[0][\"index\"]\n",
        "gesture_model_quantized_output_index = gesture_model_quantized.get_output_details()[0][\"index\"]\n",
        "\n",
        "print(gesture_model.get_input_details())\n",
        "print(gesture_model.get_output_details())\n",
        "print(gesture_model_quantized.get_input_details())\n",
        "print(gesture_model_quantized.get_output_details())\n",
        "pprint.pprint(gesture_model.get_tensor_details())\n",
        "pprint.pprint(gesture_model_quantized.get_tensor_details())\n",
        "\n",
        "predictions = model.predict(testX)\n",
        "# Create arrays to store the results\n",
        "gesture_model_predictions = []\n",
        "gesture_model_quantized_predictions = []\n",
        "\n",
        "# Run each model's interpreter for each value and store the results in arrays\n",
        "for x_value in testX:\n",
        "  # Create a 2D tensor wrapping the current x value\n",
        "  x_value_tensor = tf.convert_to_tensor([x_value], dtype=np.float32)\n",
        "  # Write the value to the input tensor\n",
        "  gesture_model.set_tensor(gesture_model_input_index, x_value_tensor)\n",
        "  # Run inference\n",
        "  gesture_model.invoke()\n",
        "  # Read the prediction from the output tensor\n",
        "  gesture_model_predictions.append(\n",
        "      gesture_model.get_tensor(gesture_model_output_index)[0])\n",
        "  # Do the same for the quantized model\n",
        "  gesture_model_quantized.set_tensor(gesture_model_quantized_input_index, x_value_tensor)\n",
        "  gesture_model_quantized.invoke()\n",
        "  gesture_model_quantized_predictions.append(\n",
        "      gesture_model_quantized.get_tensor(gesture_model_quantized_output_index)[0])\n",
        "\n",
        "# See how they line up with the data\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.clf()\n",
        "plt.title('Comparison of various models against actual values')\n",
        "plt.plot(range(len(testX)), [np.argmax(label) for label in testY] , 'bo', label='Actual')\n",
        "plt.plot(range(len(testX)), [np.argmax(label) for label in predictions], 'ro', label='Original predictions')\n",
        "plt.plot(range(len(testX)), [np.argmax(label) for label in gesture_model_predictions], 'bx', label='Lite predictions')\n",
        "plt.plot(range(len(testX)), [np.argmax(label) for label in gesture_model_quantized_predictions], 'gx', label='Lite quantized predictions')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Q4qKAgoDmR",
        "colab_type": "text"
      },
      "source": [
        "Now, we use a tool called xxd in order to export our model out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDSDg0zXQpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get -qq install xxd\n",
        "\n",
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_quantized.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "!echo \"const unsigned char model[] = {\" > /content/model_quantized.h\n",
        "!cat gesture_quantized.tflite | xxd -i      >> /content/model_quantized.h\n",
        "!echo \"};\"                              >> /content/model_quantized.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "model_quantized_h_size = os.path.getsize(\"model_quantized.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(f\"Header file, model.h, is {model_quantized_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0dp7RrDpTNT",
        "colab_type": "text"
      },
      "source": [
        "Here, you can actually clone the repository to you computer, and edit there. \n",
        "\n",
        "Here, for sanity, we are going to cat the files into the cell here, and you are going to copy it in another cell, and save it with %%writefile filename.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0lvaQYwYxw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the repository from GitHub\n",
        "!git clone --depth 1 -q https://github.com/tensorflow/tensorflow\n",
        "# Check out a commit that has been tested to work\n",
        "# with the build of TensorFlow we're using\n",
        "!git -c advice.detachedHead=false -C tensorflow checkout v2.1.0-rc0\n",
        "# the list of gestures that data is available for"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8MRLX5Ko9iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/tensorflow/lite/experimental/micro/examples/\n",
        "!cat magic_wand_test.cc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r912UyQorqh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile magic_wand_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXqNYwBDpNhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat magic_wand_mode_data.cc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M14D5ngr62e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile magic_wand_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkxpyF_VZaBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/tensorflow\n",
        "!make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=sparkfun_edge magic_wand_bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnD-UaFCn35G",
        "colab_type": "text"
      },
      "source": [
        "This command is needed because there is a mistake in the naming of the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eG_sxxbZ-JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/tools/apollo3_scripts/keys_info0.py \\\n",
        "tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/tools/apollo3_scripts/keys_info.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NXcA3KJsJD4",
        "colab_type": "text"
      },
      "source": [
        "This command is required to create a secure binary to flash into the board."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BamGXcqc9HVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pycrypto pyserial\n",
        "\n",
        "%run tensorflow/lite/experimental/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/tools/apollo3_scripts/create_cust_image_blob.py \\\n",
        "--bin tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/magic_wand.bin \\\n",
        "--load-address 0xC000 \\\n",
        "--magic-num 0xCB \\\n",
        "-o main_nonsecure_ota \\\n",
        "--version 0x0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpEgEFOHsXGR",
        "colab_type": "text"
      },
      "source": [
        "If you want to help me, tahe a look at this issue report at TFlite googlegroups.\n",
        "\n",
        "\n",
        "https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/tflite/PTcmPMOpJQ4/9IRYPpM3CwAJ"
      ]
    }
  ]
}